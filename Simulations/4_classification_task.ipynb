{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5556159",
   "metadata": {},
   "source": [
    "# Classification Task\n",
    "\n",
    "In this notebook we aim to train, validate and test a neural net to classify between:\n",
    "* one galaxy behind one foreground galaxy\n",
    "* two galaxies at the same redshift and with small angular separation (1 to 4 arcseconds)\n",
    "\n",
    "These datasets were created in notebook `1_create_simulations`.\n",
    "\n",
    "This notebook also incorporates the critical diagnostics described in [Training a Machine Learning Model](https://docs.google.com/document/d/1U17RNPqDA5uP9-M5V3ENweKig5eRZWHTpja-hh1z1MI/edit).\n",
    "\n",
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import Packages](#imports)\n",
    "2. [Load data](#load)\n",
    "3. [Process data](#process)\n",
    "    1. [Ingest the data](#ingest)\n",
    "    2. [Get dataloader](#dataloader)\n",
    "4. [Train network](#train)\n",
    "    1. [Save performance](#savePerformance)\n",
    "    2. [Predict class](#predict)\n",
    "5. [Results](#results)\n",
    "    1. [Confusion matrix](#cm)\n",
    "    2. [Learning curve](#learningCurve)\n",
    "\n",
    "\n",
    "## 1. Import Packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8960cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import process\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../Network')\n",
    "import data_utils\n",
    "import networks\n",
    "import training\n",
    "import save\n",
    "import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a22bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f74e0",
   "metadata": {},
   "source": [
    "### Aestetic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False  # enable autocomplete\n",
    "\n",
    "size_default = 1.5\n",
    "size_larger = 1.9\n",
    "sns.set(font_scale=size_default, style=\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148da288",
   "metadata": {},
   "source": [
    "## Notebook Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_processed = 1  # Set to 0 if the data has not yet been processed\n",
    "is_trained = 0  # Set to 0 if the network has not yet been trained\n",
    "\n",
    "# Select the network type\n",
    "#net_type = 'CNN'\n",
    "# net_type = 'RNN'\n",
    "net_type = 'ZIPPER'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13078df4",
   "metadata": {},
   "source": [
    "## 2. Load data<a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c542b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'high_cad_1_2_data'\n",
    "directory = dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89ce26",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 2.1. Process data<a name=\"process\"></a>\n",
    "\n",
    "Following [DeepZipper paper](https://arxiv.org/pdf/2112.01541.pdf), we condense the image information to single-image input by averaging all images in the time series on a pixel-by-pixel basis within each band.\n",
    "\n",
    "This also augments the training set by rotating and morroring the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_processed:\n",
    "    configurations = [x.split(\"/\")[1].split(\"_images.\")[0] for x in glob.glob(f\"{directory}/*_images.npy\")]\n",
    "    \n",
    "    ini_time = time.time()\n",
    "    for configuration in sorted(configurations):\n",
    "        process.run(directory, configuration)\n",
    "    print(time.time() - ini_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdf474",
   "metadata": {},
   "source": [
    "### 2.2. Original data<a name=\"loadOri\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fc31b",
   "metadata": {},
   "source": [
    "Configuration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_config1_ori = np.load(dataset_name+'/CONFIGURATION_1_images.npy')\n",
    "metadata_config1_ori = pd.read_csv(dataset_name+'/CONFIGURATION_1_metadata.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(images_config1_ori))\n",
    "print(np.shape(metadata_config1_ori))\n",
    "print('# events: ', len(np.unique(metadata_config1_ori['OBJID-g'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50efb6",
   "metadata": {},
   "source": [
    "Configuration 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_config2_ori = np.load(dataset_name+'/CONFIGURATION_2_images.npy')\n",
    "metadata_config2_ori = pd.read_csv(dataset_name+'/CONFIGURATION_2_metadata.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(images_config2_ori))\n",
    "print(np.shape(metadata_config2_ori))\n",
    "print('# events: ', len(np.unique(metadata_config2_ori['OBJID-g'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd38c8",
   "metadata": {},
   "source": [
    "### 2.3. Processed data<a name=\"loadAug\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f7277c",
   "metadata": {},
   "source": [
    "Configuration 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f08f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_config1 = np.load(dataset_name+'/CONFIGURATION_1_proc_ims_15.npy')\n",
    "metadata_config1 = np.load(dataset_name+'/CONFIGURATION_1_proc_mds_15.npy', \n",
    "                           allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(images_config1))\n",
    "print(np.shape(metadata_config1[0]))\n",
    "print('# events: ', len(metadata_config1.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8fa36",
   "metadata": {},
   "source": [
    "Configuration 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f431000",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_config2 = np.load(dataset_name+'/CONFIGURATION_2_proc_ims_15.npy')\n",
    "metadata_config2 = np.load(dataset_name+'/CONFIGURATION_2_proc_mds_15.npy', \n",
    "                           allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88309c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(images_config2))\n",
    "print(np.shape(metadata_config2[0]))\n",
    "print('# events: ', len(metadata_config2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rotation of the images\n",
    "# from deeplenstronomy.visualize import view_image, view_image_rgb\n",
    "# view_image_rgb(images_config2[1], Q=10, stretch=1)\n",
    "# view_image_rgb(images_config2[138], Q=10, stretch=1)\n",
    "# view_image_rgb(images_config2[275], Q=10, stretch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac16dae",
   "metadata": {},
   "source": [
    "## 3. Prepare data for network<a name=\"prepare\"></a>\n",
    "\n",
    "### 3.1. Train/Test set split<a name=\"split\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = data_utils.make_train_test_datasets(\n",
    "    directory=dataset_name, class_names=['CONFIGURATION_1_proc', 'CONFIGURATION_2_proc'], \n",
    "    suffix='15', label_map={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e383618",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_dataset.images) # I thought we added all the images? Why still many filters??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e6747",
   "metadata": {},
   "source": [
    "### 3.2. Get dataloader<a name=\"dataloader\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data_utils.make_dataloader(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d488fd2",
   "metadata": {},
   "source": [
    "### 3.3. Data Histogram<a name=\"dataHist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label_to_real = {0: 'gal-gal', 1: '2 gals'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb79b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(-.5, 1.5, 0.25)\n",
    "sns.histplot(data=train_dataset.labels-.25, kde=False, stat='density', \n",
    "             bins=bins,\n",
    "             color='C0', label='Train set', linewidth=3, fill=False)\n",
    "sns.histplot(data=test_dataset.labels, kde=False, stat='density', \n",
    "             bins=bins, \n",
    "             color='C1', label='Test set', linewidth=3, fill=False)\n",
    "plt.xticks(ticks=[0, 1], labels=[dict_label_to_real[0], dict_label_to_real[1]])\n",
    "plt.xlim(-.5, 1.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ce6ac",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "## 4. Train network<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_types = {'ZIPPER': networks.ZipperNN(4, 4, 4),\n",
    "                 'CNN': networks.CNN_single(4, 2),\n",
    "                 'RNN': networks.RNN_single(4, 3)}\n",
    "\n",
    "network = network_types[net_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_trained:\n",
    "    ini_time = time.time()\n",
    "    if net_type == 'ZIPPER':\n",
    "        network = training.train_zipper(network, train_dataloader,\n",
    "                                        train_dataset, test_dataset,\n",
    "                                        monitor=True,\n",
    "                                        outfile_prefix=f\"{dataset_name}/{dataset_name}_{net_type}\")\n",
    "    else:\n",
    "        if net_type == 'CNN':\n",
    "            datatype = 'image'\n",
    "        elif net_type == 'RNN':\n",
    "            datatype = 'lightcurve'\n",
    "        else:\n",
    "            raise ValueError('`net_type` {net_type} not recognised')\n",
    "        network = training.train_single(network, train_dataloader, \n",
    "                                    train_dataset, test_dataset,\n",
    "                                    datatype, monitor=True,\n",
    "                                    outfile_prefix=f\"{dataset_name}/{dataset_name}_{net_type}\")\n",
    "    print(time.time() - ini_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73a54e",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 4.1. Save performance<a name=\"savePerformance\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f45801",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_trained:\n",
    "    print(\"Saving results\")\n",
    "    # Save the performance\n",
    "    save.save_performance(dataset_name, dataset_name, net_type, network, test_dataset)\n",
    "    save.save_performance(dataset_name, dataset_name, net_type, network, train_dataset, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8de01",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 4.2. Load network<a name=\"loadNetwork\"></a>\n",
    "\n",
    "If the network was previously ran, simply load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ed03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_table = pd.read_csv(directory+f'/{directory}_{net_type}_monitoring.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_trained:\n",
    "    network = network_types[net_type]\n",
    "    network.load_state_dict(torch.load(directory+f'/{directory}_{net_type}_network.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbc133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6480b6",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 4.3. Predict class<a name=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f209865",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = predict.predict(network, test_dataset) \n",
    "accuracy = np.sum(predictions == labels) / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4d8fb",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "## 5. Results<a name=\"results\"></a>\n",
    "\n",
    "### 5.1. Confusion matrix<a name=\"cm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f85543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title=None, normalise=None,\n",
    "                          dict_label_to_real=None, figsize=None, **kwargs):\n",
    "    \"\"\"Plot a confusion matrix.\n",
    "\n",
    "    Uses the true and predicted class labels to compute a confusion matrix.\n",
    "    This can be non-normalised, normalised by true class/row (the diagonals\n",
    "    show the accuracy of each class), and by predicted class/column (the\n",
    "    diagonals show the precision).\n",
    "    \n",
    "    This code is from snmachine: https://github.com/LSSTDESC/snmachine\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1D array-like\n",
    "        Ground truth (correct) labels of shape (n_samples,).\n",
    "    y_true : 1D array-like\n",
    "        Predicted class labels of shape (n_samples,).\n",
    "    title : {None, str}, optional\n",
    "        Title of the plot.\n",
    "    normalise : {None, str}, optional\n",
    "       If `None`, use the absolute numbers in each matrix entry. If 'accuracy',\n",
    "       normalise per true class. If 'precision', normalise per predicted class.\n",
    "    dict_label_to_real : dict, optional\n",
    "        Dictionary containing the class labels as key and its real name as\n",
    "        values. E.g. for PLAsTiCC\n",
    "        `dict_label_to_real = {42: 'SNII', 62: 'SNIbc', 90: 'SNIa'}`.\n",
    "        If `None`, the default class labels are used.\n",
    "    figsize : {None, tuple}\n",
    "        If `None`, use the default `figsize` of the plot. Otherwise, create a\n",
    "        figure with the given size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cm : np.array\n",
    "       The confusion matrix, as computed by `sklearn.metrics.confusion_matrix`.\n",
    "    \"\"\"\n",
    "    # Make and normalise the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalise == 'accuracy':\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        kwargs = {'vmin': 0, 'vmax': 1}\n",
    "        print(\"Confusion matrix normalised by true class.\")\n",
    "    elif normalise == 'precision':\n",
    "        cm = cm.astype('float') / cm.sum(axis=0)[np.newaxis, :]\n",
    "        kwargs = {'vmin': 0, 'vmax': 1}\n",
    "        print(\"Confusion matrix normalised by predicted class.\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalisation')\n",
    "\n",
    "    # Classes in the dataset\n",
    "    target_names = np.unique(y_true)\n",
    "    target_names_ori = np.copy(target_names)  # the labels might be strings\n",
    "    if dict_label_to_real is not None:\n",
    "        target_names = np.vectorize(dict_label_to_real.get)(target_names)\n",
    "        if target_names[0] is None:  # fix the names being strings\n",
    "            target_names = list(map(int, target_names_ori))\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    if figsize is not None:\n",
    "        _, ax = plt.subplots(figsize=figsize)  # good values: (9, 7)\n",
    "    else:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.heatmap(cm, xticklabels=target_names,\n",
    "                yticklabels=target_names, cmap='Blues',\n",
    "                annot=True, fmt='.2f', lw=0.5,\n",
    "                cbar_kws={'label': 'Fraction of events',\n",
    "                          'shrink': .82}, **kwargs)\n",
    "    ax.set_xlabel('Predicted class')\n",
    "    ax.set_ylabel('True class')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    ax.set_aspect('equal')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6661520",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=labels, y_pred=predictions, \n",
    "                      normalise='accuracy', \n",
    "                      title=net_type+f'\\nAccuracy: {accuracy:.3f}', \n",
    "                      dict_label_to_real={0:'gal-gal', 1:'2 gals'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7195ec76",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)\n",
    "\n",
    "### 5.2. Learning curve<a name=\"learningCurve\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b085d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monitor_table['Loss'], linewidth=3, label='Loss')\n",
    "plt.plot(monitor_table['Train Acc'], linewidth=3, label='Train Accuracy')\n",
    "plt.plot(monitor_table['Test Acc'], linewidth=3, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.title('Learning curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d2d51",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
